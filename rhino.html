
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta name=viewport content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .new {
    background-color : #cc0000; color:white; border-radius : 5px; padding :1px; font-size : 14px; margin : 0 5px;}








    </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>rhino</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86789415-1', 'auto');
  ga('send', 'pageview');









    </script>
</head>
<body>
<table width="840" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Reconstruction witH dIfferNtial geOmetry (RHINO)</name>
                        </p>

                        <p align="center">
                            <name>ANR JCJC (2022-2026)</name>
                        </p>

                        <p align="left">
                            <name>Our Goal</name>
                        </p>

                        <!-- ######## ABSTRACT #########-->

                        <p style="text-align:justify">
                            3D Reconstruction from multiple images has always been a major research goal in computer vision but it has gained tremendous attention from the research community in recent times. Given a camera attached to a stationary/hand-held device or a robot, the ability to obtain the 3D scene from its video stream, or, alternatively, from a 3D sensor such as Kinect, Time of Flight camera, stereo rig or a LIDAR, opens doors to numerous applications. Application-wise, the obtained 3D scene can be used to develop mixed reality applications including remote surgery, virtual classrooms, enhanced shopping experiences, assistance in autonomous driving and immersive gaming experiences. A robot capable of autonomously navigating and interacting within human spaces will need to be able to perceive the 3D world. Currently, the development of these applications is limited to rigid environments, as the current state-of-the-art in 3D reconstruction is unable to reliably process deformable objects. On the other hand, our world is dynamic, it consists of both rigid and deformable objects, whose reconstruction from cameras and 3D sensors must be made as accurate and stable as possible. The goal of RHINO is to bridge the performance gap between deformable and rigid 3D reconstruction using cameras and 3D sensors so that the scientific community can move forward with developing mixed reality or robotics applications that work on real-life scenarios.
                            
                            
                        
                        </p>

                        <p align="left">
                            <name>Our approach</name>
                        </p>

                        <!-- ######## ABSTRACT #########-->

                        <p style="text-align:justify">
                            3D Reconstruction from multiple images has always been a major research goal in computer vision but it has gained tremendous attention from the research community in recent times. Given a camera attached to a stationary/hand-held device or a robot, the ability to obtain the 3D scene from its video stream, or, alternatively, from a 3D sensor such as Kinect, Time of Flight camera, stereo rig or a LIDAR, opens doors to numerous applications. Application-wise, the obtained 3D scene can be used to develop mixed reality applications including remote surgery, virtual classrooms, enhanced shopping experiences, assistance in autonomous driving and immersive gaming experiences. A robot capable of autonomously navigating and interacting within human spaces will need to be able to perceive the 3D world. Currently, the development of these applications is limited to rigid environments, as the current state-of-the-art in 3D reconstruction is unable to reliably process deformable objects. On the other hand, our world is dynamic, it consists of both rigid and deformable objects, whose reconstruction from cameras and 3D sensors must be made as accurate and stable as possible. The goal of RHINO is to bridge the performance gap between deformable and rigid 3D reconstruction using cameras and 3D sensors so that the scientific community can move forward with developing mixed reality or robotics applications that work on real-life scenarios.
                            
                            
                        
                        </p>

                        <p align="left">
                            <name>Our team</name>
                                    <li>
                                        2023-2026: Thuy Tran. 
                                    </li>
                                        
                                    <li>
                                        2022-2025: Ruochen Chen. 
                                    </li>
                        </p>

                        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <td>
                    <heading>Publications</heading>
                </td>

                <tbody>

                    <tr>
                    <td width="25%"><img src="images/poly.jpg" alt="blind-date"
                                         width="200"
                                         height="130"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/pdf/2601.05035"> 
                                <papertitle>Patch-based Representation and Learning for Efficient Deformation Modeling
                                </papertitle>
                            </a>
                            <br>
                            Ruochen Chen, Thuy Tran, <strong>Shaifali Parashar</strong>
                            <br>
                            <em>3DV</em>, 2026 &nbsp; <font color="red"><strong>(Oral, award candidate)</strong></font>
                            <br>
                            <a href="https://github.com/Simonhfls/PolyFit">Code</a>
                        </p>
    
                    </td>
                </tr>

                    <tr>
                    <td width="25%"><img src="images/fnopt.jpg" alt="blind-date"
                                         width="200"
                                         height="130"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/pdf/2512.05762"> 
                                <papertitle>FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using
Meta-Optimization with Fourier Neural Operators
                                </papertitle>
                            </a>
                            <br>
                            Ruochen Chen, Thuy Tran, <strong>Shaifali Parashar</strong>
                            <br>
                            <em>WACV</em>, 2026 &nbsp;
                            <br>
                            <a href="https://github.com/Simonhfls/FNOpt">Code</a>
                        </p>
    
                    </td>
                </tr>
                    
                    <tr>
                    <td width="25%"><img src="https://github.com/dvttran/nsft/raw/main/assets/nsft_teaser.jpeg" alt="blind-date"
                                         width="200"
                                         height="120"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/abs/2507.22699"> 
                                <papertitle>Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints
                                </papertitle>
                            </a>
                            <br>
                            Thuy Tran, Ruochen Chen, <strong>Shaifali Parashar</strong>
                            <br>
                            <em>ICCV</em>, 2025 &nbsp;
                            <br>
                            <a href="https://github.com/dvttran/nsft">Code</a>
                        </p>
    
                    </td>
                </tr>
                        </tbody>
                       
            <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}



            </script>
        </td>
    </tr>
</table>
</body>
</html>

